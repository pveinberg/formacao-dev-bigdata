{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, dataframe\n",
    "from pyspark.sql.functions import when, col, sum, count, isnan, round\n",
    "from pyspark.sql.functions import regexp_replace, concat_ws, sha2, rtrim\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "from pyspark.sql.functions import countDistinct \n",
    "from pyspark.sql.functions import year, month, dayofmonth, quarter \n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.Criar dataframes das tabelas que estão no hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+-------------+--------+\n",
      "|id_categoria|   ds_categoria|perc_parceiro| dt_foto|\n",
      "+------------+---------------+-------------+--------+\n",
      "|           1|Categoria - 001|          2.0|20230603|\n",
      "|           2|Categoria - 002|          2.0|20230603|\n",
      "|           3|Categoria - 003|          2.0|20230603|\n",
      "|           4|Categoria - 004|          2.0|20230603|\n",
      "|           5|Categoria - 005|          5.0|20230603|\n",
      "|           6|Categoria - 006|          1.0|20230603|\n",
      "|           7|Categoria - 007|          5.0|20230603|\n",
      "|           8|Categoria - 008|          3.0|20230603|\n",
      "|           9|Categoria - 009|          5.0|20230603|\n",
      "|          10|Categoria - 010|          6.0|20230603|\n",
      "|          11|Categoria - 011|          6.0|20230603|\n",
      "|          12|Categoria - 012|          4.0|20230603|\n",
      "|          13|Categoria - 013|          3.0|20230603|\n",
      "|          14|Categoria - 014|          1.0|20230603|\n",
      "|          15|Categoria - 015|          4.0|20230603|\n",
      "|          16|Categoria - 016|          3.0|20230603|\n",
      "|          17|Categoria - 017|          5.0|20230603|\n",
      "|          18|Categoria - 018|          1.0|20230603|\n",
      "|          19|Categoria - 019|          2.0|20230603|\n",
      "|          20|Categoria - 020|          5.0|20230603|\n",
      "+------------+---------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela Categorias\n",
    "df_categorias = spark.sql(\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM aula_hive_stg.tbl_categoria\n",
    "                    WHERE id_categoria != 'id_categoria'\n",
    "                    \"\"\")\n",
    "df_categorias.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------+--------+\n",
      "|id_cidade|       ds_cidade|id_estado| dt_foto|\n",
      "+---------+----------------+---------+--------+\n",
      "|     1058|           Betim|        1|20230603|\n",
      "|       33|      ACRELANDIA|        2|20230603|\n",
      "|      485|    ASSIS BRASIL|        2|20230603|\n",
      "|      958|       BRASILEIA|        2|20230603|\n",
      "|     1388|        CAPIXABA|        2|20230603|\n",
      "|     1851| CRUZEIRO DO SUL|        2|20230603|\n",
      "|     2022|       Cravinhos|        2|20230603|\n",
      "|     2232|  EPITACIOLANDIA|        2|20230603|\n",
      "|     2347|           FEIJO|        2|20230603|\n",
      "|     3879|     MANCIO LIMA|        2|20230603|\n",
      "|     3895|   MANOEL URBANO|        2|20230603|\n",
      "|     5628|      RIO BRANCO|        2|20230603|\n",
      "|     5767|      Rio Branco|        2|20230603|\n",
      "|     6585|  SENA MADUREIRA|        2|20230603|\n",
      "|     6593|SENADOR GUIOMARD|        2|20230603|\n",
      "|     6845|       SÃO PAULO|        2|20230603|\n",
      "|     6987|        TARAUACA|        2|20230603|\n",
      "|     7241|      UBERLANDIA|        2|20230603|\n",
      "|     7535|          XAPURI|        2|20230603|\n",
      "|       59|     AGUA BRANCA|        3|20230603|\n",
      "+---------+----------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela cidade\n",
    "df_cidade = spark.sql(\"\"\"\n",
    "                SELECT *\n",
    "                FROM aula_hive_stg.tbl_cidade\n",
    "                WHERE id_cidade != 'id_cidade'\n",
    "                \"\"\")\n",
    "df_cidade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+\n",
      "|id_estado|ds_estado| dt_foto|\n",
      "+---------+---------+--------+\n",
      "|        1|       31|20230603|\n",
      "|        2|       AC|20230603|\n",
      "|        3|       AL|20230603|\n",
      "|        4|       AM|20230603|\n",
      "|        5|       AP|20230603|\n",
      "|        6|       BA|20230603|\n",
      "|        7|       CE|20230603|\n",
      "|        8|       DF|20230603|\n",
      "|        9|       Df|20230603|\n",
      "|       10|       ES|20230603|\n",
      "|       11|       GO|20230603|\n",
      "|       12|       Go|20230603|\n",
      "|       13|       MA|20230603|\n",
      "|       14|       MG|20230603|\n",
      "|       15|       MS|20230603|\n",
      "|       16|       MT|20230603|\n",
      "|       17|       Mg|20230603|\n",
      "|       18|       PA|20230603|\n",
      "|       19|       PB|20230603|\n",
      "|       20|       PE|20230603|\n",
      "+---------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela estado\n",
    "df_estado = spark.sql(\"\"\"\n",
    "                SELECT *\n",
    "                FROM aula_hive_stg.tbl_estado\n",
    "                WHERE id_estado != 'id_estado'\n",
    "                \"\"\")\n",
    "df_estado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+--------+\n",
      "|id_cliente|          nm_cliente|flag_ouro| dt_foto|\n",
      "+----------+--------------------+---------+--------+\n",
      "|  78262350|Cliente Magalu - ...|        0|20230603|\n",
      "|  57301020|Cliente Magalu - ...|        0|20230603|\n",
      "|   4639167|Cliente Magalu - ...|        1|20230603|\n",
      "|  53130287|Cliente Magalu - ...|        0|20230603|\n",
      "|  16456085|Cliente Magalu - ...|        0|20230603|\n",
      "| 124100202|Cliente Magalu - ...|        0|20230603|\n",
      "|  15659567|Cliente Magalu - ...|        0|20230603|\n",
      "|  37375617|Cliente Magalu - ...|        1|20230603|\n",
      "|  52082515|Cliente Magalu - ...|        0|20230603|\n",
      "|  56214875|Cliente Magalu - ...|        0|20230603|\n",
      "| 137980527|Cliente Magalu - ...|        0|20230603|\n",
      "|  11806492|Cliente Magalu - ...|        0|20230603|\n",
      "| 112690290|Cliente Magalu - ...|        1|20230603|\n",
      "| 120559842|Cliente Magalu - ...|        1|20230603|\n",
      "| 140697495|Cliente Magalu - ...|        0|20230603|\n",
      "|  64431627|Cliente Magalu - ...|        0|20230603|\n",
      "| 113852445|Cliente Magalu - ...|        1|20230603|\n",
      "|  51864985|Cliente Magalu - ...|        0|20230603|\n",
      "| 113068442|Cliente Magalu - ...|        0|20230603|\n",
      "|  71256730|Cliente Magalu - ...|        0|20230603|\n",
      "+----------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela clientes\n",
    "df_clientes = spark.sql(\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM aula_hive_stg.tbl_cliente\n",
    "                    WHERE id_cliente != 'id_cliente'\n",
    "                    \"\"\")\n",
    "df_clientes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------+--------+\n",
      "|id_filial|      ds_filial|id_cidade| dt_foto|\n",
      "+---------+---------------+---------+--------+\n",
      "|        6|Filial - 000006|       22|20230603|\n",
      "|        9|Filial - 000009|       22|20230603|\n",
      "|       88|Filial - 000088|       22|20230603|\n",
      "|       98|Filial - 000098|       22|20230603|\n",
      "|      118|Filial - 000118|       22|20230603|\n",
      "|      146|Filial - 000146|       22|20230603|\n",
      "|      223|Filial - 000223|       22|20230603|\n",
      "|      268|Filial - 000268|       22|20230603|\n",
      "|      353|Filial - 000353|       22|20230603|\n",
      "|      363|Filial - 000363|       22|20230603|\n",
      "|      372|Filial - 000372|       22|20230603|\n",
      "|      373|Filial - 000373|       22|20230603|\n",
      "|      417|Filial - 000417|       22|20230603|\n",
      "|      464|Filial - 000464|       22|20230603|\n",
      "|      470|Filial - 000470|       22|20230603|\n",
      "|      531|Filial - 000531|       22|20230603|\n",
      "|      538|Filial - 000538|       22|20230603|\n",
      "|      617|Filial - 000617|       22|20230603|\n",
      "|      620|Filial - 000620|       22|20230603|\n",
      "|      625|Filial - 000625|       22|20230603|\n",
      "+---------+---------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela filial\n",
    "df_filial = spark.sql(\"\"\"\n",
    "                SELECT *\n",
    "                FROM aula_hive_stg.tbl_filial\n",
    "                WHERE id_filial != 'id_filial'\n",
    "                \"\"\")\n",
    "df_filial.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+-----------+--------+\n",
      "|  id_pedido|id_produto|quantidade|vr_unitario| dt_foto|\n",
      "+-----------+----------+----------+-----------+--------+\n",
      "| 4972773226|   1357853|         1|    9998.17|20230603|\n",
      "| 4782631506|   5017608|         1|     999.57|20230603|\n",
      "| 4793147056|    721095|         1|     999.57|20230603|\n",
      "| 4800263156|    721095|         1|     999.57|20230603|\n",
      "| 4810312456|    721095|         1|     999.57|20230603|\n",
      "| 4770011776|   5017608|         1|     999.57|20230603|\n",
      "|48935276716|   3277163|         8|      998.3|20230603|\n",
      "| 4896805005|   2282017|         1|     998.27|20230603|\n",
      "| 4748781753|     81781|         1|     996.97|20230603|\n",
      "| 4708762423|   1158116|         1|     996.97|20230603|\n",
      "| 4747737453|     21525|         1|     996.97|20230603|\n",
      "| 4727352703|   2589693|         1|     996.97|20230603|\n",
      "| 4748424453|     21525|         1|     996.97|20230603|\n",
      "| 4748822373|   1831256|         1|     996.97|20230603|\n",
      "| 4688052453|   1831256|         1|     996.97|20230603|\n",
      "| 4796749703|   3515732|         1|     996.97|20230603|\n",
      "| 4732648053|   1147454|         1|     996.97|20230603|\n",
      "| 4782896773|   4942308|         1|     996.97|20230603|\n",
      "| 4775369073|    998717|         1|     996.97|20230603|\n",
      "| 4833150473|   4975601|         1|     996.97|20230603|\n",
      "+-----------+----------+----------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela item_pedido\n",
    "df_item_pedido = spark.sql(\"\"\"\n",
    "                        SELECT *\n",
    "                        FROM aula_hive_stg.tbl_item_pedido\n",
    "                        WHERE id_pedido != 'id_pedido'\n",
    "                        ORDER BY vr_unitario DESC\n",
    "                        \"\"\")\n",
    "df_item_pedido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+\n",
      "|id_parceiro|         nm_parceiro| dt_foto|\n",
      "+-----------+--------------------+--------+\n",
      "|          1|Parceiro Magalu - 01|20230603|\n",
      "|          2|Parceiro Magalu - 02|20230603|\n",
      "|          3|Parceiro Magalu - 03|20230603|\n",
      "|          4|Parceiro Magalu - 04|20230603|\n",
      "|          5|Parceiro Magalu - 05|20230603|\n",
      "|          6|Parceiro Magalu - 06|20230603|\n",
      "|          7|Parceiro Magalu - 07|20230603|\n",
      "|          8|Parceiro Magalu - 08|20230603|\n",
      "|          9|Parceiro Magalu - 09|20230603|\n",
      "|         10|Parceiro Magalu - 10|20230603|\n",
      "|         11|Parceiro Magalu - 11|20230603|\n",
      "|         12|Parceiro Magalu - 12|20230603|\n",
      "|         13|Parceiro Magalu - 13|20230603|\n",
      "|         14|Parceiro Magalu - 14|20230603|\n",
      "|         15|Parceiro Magalu - 15|20230603|\n",
      "|         16|Parceiro Magalu - 16|20230603|\n",
      "+-----------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela parceiro\n",
    "df_parceiro = spark.sql(\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM aula_hive_stg.tbl_parceiro\n",
    "                    WHERE id_parceiro != 'id_parceiro'\n",
    "                    \"\"\")\n",
    "df_parceiro.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------+--------+\n",
      "|id_produto|          ds_produto|id_subcategoria| dt_foto|\n",
      "+----------+--------------------+---------------+--------+\n",
      "|     12006|Produto - 0000012006|            572|20230603|\n",
      "|     45183|Produto - 0000045183|           2142|20230603|\n",
      "|     78905|Produto - 0000078905|            140|20230603|\n",
      "|     79758|Produto - 0000079758|           3132|20230603|\n",
      "|    117196|Produto - 0000117196|             96|20230603|\n",
      "|    169903|Produto - 0000169903|           1735|20230603|\n",
      "|    185406|Produto - 0000185406|             82|20230603|\n",
      "|    201415|Produto - 0000201415|           1519|20230603|\n",
      "|    284396|Produto - 0000284396|           3132|20230603|\n",
      "|    293376|Produto - 0000293376|           1309|20230603|\n",
      "|    299033|Produto - 0000299033|            336|20230603|\n",
      "|    301419|Produto - 0000301419|           2370|20230603|\n",
      "|    312382|Produto - 0000312382|           1610|20230603|\n",
      "|    315401|Produto - 0000315401|           2880|20230603|\n",
      "|    316342|Produto - 0000316342|           1236|20230603|\n",
      "|    335667|Produto - 0000335667|           3228|20230603|\n",
      "|    344427|Produto - 0000344427|           2925|20230603|\n",
      "|    384651|Produto - 0000384651|           3132|20230603|\n",
      "|    384922|Produto - 0000384922|           2300|20230603|\n",
      "|    389046|Produto - 0000389046|            314|20230603|\n",
      "+----------+--------------------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela produtos\n",
    "df_produtos = spark.sql(\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM aula_hive_stg.tbl_produto\n",
    "                    WHERE id_produto != 'id_produto'\n",
    "                    \"\"\")\n",
    "df_produtos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------------+--------+\n",
      "|id_subcategoria|     ds_subcategoria|id_categoria| dt_foto|\n",
      "+---------------+--------------------+------------+--------+\n",
      "|            132|Sub-categoria - 0...|           1|20230603|\n",
      "|            137|Sub-categoria - 0...|           1|20230603|\n",
      "|            288|Sub-categoria - 0...|           1|20230603|\n",
      "|            380|Sub-categoria - 0...|           1|20230603|\n",
      "|            397|Sub-categoria - 0...|           1|20230603|\n",
      "|            498|Sub-categoria - 0...|           1|20230603|\n",
      "|            508|Sub-categoria - 0...|           1|20230603|\n",
      "|            514|Sub-categoria - 0...|           1|20230603|\n",
      "|            586|Sub-categoria - 0...|           1|20230603|\n",
      "|            668|Sub-categoria - 0...|           1|20230603|\n",
      "|            751|Sub-categoria - 0...|           1|20230603|\n",
      "|            923|Sub-categoria - 0...|           1|20230603|\n",
      "|           1040|Sub-categoria - 0...|           1|20230603|\n",
      "|           1170|Sub-categoria - 0...|           1|20230603|\n",
      "|           1211|Sub-categoria - 0...|           1|20230603|\n",
      "|           1426|Sub-categoria - 0...|           1|20230603|\n",
      "|           1459|Sub-categoria - 0...|           1|20230603|\n",
      "|           1466|Sub-categoria - 0...|           1|20230603|\n",
      "|           1483|Sub-categoria - 0...|           1|20230603|\n",
      "|           1486|Sub-categoria - 0...|           1|20230603|\n",
      "+---------------+--------------------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela subcategoria\n",
    "df_subcategoria = spark.sql(\"\"\"\n",
    "                        SELECT *\n",
    "                        FROM aula_hive_stg.tbl_subcategoria\n",
    "                        WHERE id_subcategoria != 'id_subcategoria'\n",
    "                        \"\"\")\n",
    "df_subcategoria.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+----------+---------+-------------+--------+\n",
      "|  id_pedido|           dt_pedido|id_parceiro|id_cliente|id_filial|vr_total_pago| dt_foto|\n",
      "+-----------+--------------------+-----------+----------+---------+-------------+--------+\n",
      "| 4972773226|2021-08-08T00:00:...|          6|  10176110|        9|      9998.17|20230603|\n",
      "| 4713322026|2021-06-13T00:00:...|          6|  54099842|      329|      9992.97|20230603|\n",
      "|48387711216|2021-07-10T00:00:...|         16| 141774350|        3|       999.69|20230603|\n",
      "|49280204216|2021-07-29T00:00:...|         16|  34170242|      231|       999.66|20230603|\n",
      "| 4770011776|2021-06-25T00:00:...|          6|   1674237|      371|       999.57|20230603|\n",
      "| 4810312456|2021-07-05T00:00:...|          6| 121001967|        3|       999.57|20230603|\n",
      "| 4793147056|2021-06-30T00:00:...|          6| 141065652|      257|       999.57|20230603|\n",
      "| 4782631506|2021-06-28T00:00:...|          6|  45642457|      371|       999.57|20230603|\n",
      "| 4800263156|2021-07-02T00:00:...|          6| 141178977|        3|       999.57|20230603|\n",
      "| 5048954826|2021-08-24T00:00:...|          6|  52529262|        4|      9981.92|20230603|\n",
      "| 4858971826|2021-07-14T00:00:...|          6| 136491410|        3|      9981.92|20230603|\n",
      "|47113787716|2021-06-12T00:00:...|         16| 131242960|        3|       998.34|20230603|\n",
      "|47141292016|2021-06-13T00:00:...|         16| 136341462|      547|       998.34|20230603|\n",
      "|47195385216|2021-06-14T00:00:...|         16|  10477642|      231|       998.34|20230603|\n",
      "| 4896805005|2021-07-22T00:00:...|          5|  59837370|      411|       998.27|20230603|\n",
      "|46905293016|2021-06-08T00:00:...|         16| 131604087|      547|        998.2|20230603|\n",
      "|51507822016|2021-09-15T00:00:...|         16|    575467|        3|        998.2|20230603|\n",
      "|46818578516|2021-06-06T00:00:...|         16| 139253795|      276|        998.2|20230603|\n",
      "|46963563716|2021-06-09T00:00:...|         16|  71345940|        3|        998.2|20230603|\n",
      "|48634560716|2021-07-15T00:00:...|         16|  43607512|      526|       997.96|20230603|\n",
      "+-----------+--------------------+-----------+----------+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela pedido\n",
    "df_pedido = spark.sql(\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM aula_hive_stg.tbl_pedido\n",
    "                    WHERE id_pedido != 'id_pedido'\n",
    "                    ORDER BY vr_total_pago DESC\n",
    "                    \"\"\")\n",
    "df_pedido.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.Criar um dataframe (df_pedidos) esta dataframe deve ser a união de pedido e item_pedido;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pedido.createOrReplaceTempView(\"pedido\")\n",
    "df_item_pedido.createOrReplaceTempView(\"item_pedido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+----------+---------+-------------+--------+----------+----------+-----------+\n",
      "|  id_pedido|           dt_pedido|id_parceiro|id_cliente|id_filial|vr_total_pago| dt_foto|id_produto|quantidade|vr_unitario|\n",
      "+-----------+--------------------+-----------+----------+---------+-------------+--------+----------+----------+-----------+\n",
      "| 4706802305|2021-06-11T00:00:...|          5|  75664277|      141|       410.67|20230603|    390538|         9|      45.63|\n",
      "| 5080690026|2021-08-31T00:00:...|          6| 146166857|      329|      8939.97|20230603|   4355238|         9|     993.33|\n",
      "|49403134213|2021-08-01T00:00:...|         13| 102149515|        3|      2377.91|20230603|    728063|         9|     233.88|\n",
      "|49345371213|2021-07-31T00:00:...|         13|  73656702|        3|      2420.82|20230603|   1064898|         9|     268.98|\n",
      "| 4713322026|2021-06-13T00:00:...|          6|  54099842|      329|      9992.97|20230603|   4355238|         9|    1110.33|\n",
      "| 4692113575|2021-06-08T00:00:...|          5|  75664277|      224|       410.67|20230603|    390538|         9|      45.63|\n",
      "| 4901134126|2021-07-23T00:00:...|          6|  16396770|     1584|     15889.77|20230603|     84657|         9|    1765.53|\n",
      "| 4771746175|2021-06-26T00:00:...|          5|  50996950|     1695|      1253.07|20230603|     62547|         9|     139.23|\n",
      "| 4901534006|2021-07-23T00:00:...|          6| 140434155|      430|      51491.7|20230603|   4855719|         9|     5721.3|\n",
      "|51463437013|2021-09-14T00:00:...|         13| 115182460|      469|      2353.02|20230603|   2188722|         9|     222.18|\n",
      "|48420184213|2021-07-11T00:00:...|         13|  37140355|      366|     26323.92|20230603|   4194128|         9|    2924.88|\n",
      "|48758797513|2021-07-18T00:00:...|         13|  55117005|        3|      1051.92|20230603|   1082672|         9|     116.88|\n",
      "| 5076708506|2021-08-31T00:00:...|          6|  18467580|      231|     19469.97|20230603|     37656|         9|    2163.33|\n",
      "| 4706807305|2021-06-11T00:00:...|          5|  75664277|      141|       410.67|20230603|    390538|         9|      45.63|\n",
      "|50704569713|2021-08-29T00:00:...|         13| 145992110|      494|      2104.92|20230603|   4624697|         9|     233.88|\n",
      "| 4985457055|2021-08-11T00:00:...|          5| 127278140|        3|       937.17|20230603|   2174820|         9|     104.13|\n",
      "| 4692128155|2021-06-08T00:00:...|          5|  75664277|      224|       410.67|20230603|    390538|         9|      45.63|\n",
      "|49679826013|2021-08-07T00:00:...|         13| 144062027|     1738|      7896.42|20230603|   4847259|         9|     877.38|\n",
      "|50679726716|2021-08-29T00:00:...|         16|  16798150|      231|      3895.02|20230603|   3358286|         9|     432.78|\n",
      "|49721740710|2021-08-08T00:00:...|         10| 123020302|      488|       2828.0|20230603|   3176625|         8|      353.5|\n",
      "+-----------+--------------------+-----------+----------+---------+-------------+--------+----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT\n",
    "            P.*,\n",
    "            IP.id_produto,\n",
    "            IP.quantidade,\n",
    "            IP.vr_unitario\n",
    "        FROM\n",
    "            pedido as P\n",
    "        JOIN item_pedido AS IP\n",
    "        ON\n",
    "            P.id_pedido = IP.id_pedido\n",
    "        ORDER BY quantidade DESC\n",
    "        \"\"\"\n",
    "\n",
    "df_pedidos = spark.sql(query)\n",
    "\n",
    "df_pedidos.filter(df_pedidos[\"id_pedido\"] != \"id_pedido\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.Ver a quantidade de pedidos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o494.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Stream is corrupted \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:470) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) \tat org.apache.spark.scheduler.Task.run(Task.scala:121) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Stream is corrupted \tat net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:202) \tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) \tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:170) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:361) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.util.Utils$.copyStream(Utils.scala:369) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:462) \t... 23 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5daccd8e3f3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqtd_pedidos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pedido\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id_pedido\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"A quantidade de pedidos é de:{qtd_pedidos}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o494.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 32 (count at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Stream is corrupted \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:470) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) \tat org.apache.spark.scheduler.Task.run(Task.scala:121) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Stream is corrupted \tat net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:202) \tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) \tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:170) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:361) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.util.Utils$.copyStream(Utils.scala:369) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:462) \t... 23 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "qtd_pedidos = df_pedidos.select(\"id_pedido\").distinct().count()\n",
    "print(f\"A quantidade de pedidos é de:{qtd_pedidos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.Quantidade de produtos e agrupa-los por pedido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|  id_pedido|quantidade_produtos|\n",
      "+-----------+-------------------+\n",
      "|49001989016|               50.0|\n",
      "|48013645016|               45.0|\n",
      "|48483039513|               40.0|\n",
      "|47272431213|               40.0|\n",
      "|48704946213|               40.0|\n",
      "|48265171013|               36.0|\n",
      "|47769750516|               32.0|\n",
      "|50903766713|               31.0|\n",
      "|48474595213|               30.0|\n",
      "|48456340013|               30.0|\n",
      "|47741988510|               30.0|\n",
      "|49910739213|               30.0|\n",
      "|48813362513|               30.0|\n",
      "|48112853213|               30.0|\n",
      "|48394480213|               30.0|\n",
      "|51367310716|               30.0|\n",
      "|49284138513|               30.0|\n",
      "|46972298716|               30.0|\n",
      "|49235898013|               30.0|\n",
      "|49630885213|               28.0|\n",
      "+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produtos_pedido = (df_pedidos\n",
    "                   .groupby(\"id_pedido\")\n",
    "                   .agg(sum(\"quantidade\")\n",
    "                        .alias(\"quantidade_produtos\"))\n",
    "                   .orderBy(\"quantidade_produtos\", ascending=False))\n",
    "produtos_pedido.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.Quantidade de pedidos por cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|id_cliente|pedidos_por_cliente|\n",
      "+----------+-------------------+\n",
      "|  19912727|                182|\n",
      "| 133452557|                174|\n",
      "| 125627750|                120|\n",
      "|  13565460|                112|\n",
      "|  20325327|                 97|\n",
      "|  32115700|                 76|\n",
      "|  10344587|                 68|\n",
      "|   4806975|                 67|\n",
      "|   1501190|                 66|\n",
      "|  90954057|                 66|\n",
      "|  97294812|                 65|\n",
      "| 105584482|                 58|\n",
      "| 119699252|                 55|\n",
      "|  41453777|                 53|\n",
      "|  62353192|                 52|\n",
      "| 124140150|                 50|\n",
      "|  54170967|                 47|\n",
      "|  16352982|                 47|\n",
      "| 138871907|                 46|\n",
      "|  65586217|                 45|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qtd_pedido_cliente = (df_pedidos\n",
    "                      .groupby(\"id_cliente\")\n",
    "                      .agg(countDistinct(\"id_pedido\")\n",
    "                           .alias(\"pedidos_por_cliente\"))\n",
    "                      .orderBy(\"pedidos_por_cliente\", ascending=False))\n",
    "qtd_pedido_cliente.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.Quantidade de pedidos por parceiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|id_parceiro|pedidos_parceiro|\n",
      "+-----------+----------------+\n",
      "|         16|          332413|\n",
      "|         13|          306170|\n",
      "|          6|           62142|\n",
      "|          5|           19193|\n",
      "|         10|            8822|\n",
      "|          3|            3408|\n",
      "|          1|             705|\n",
      "|         11|             446|\n",
      "|          4|              88|\n",
      "|          8|              44|\n",
      "|          2|              40|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qtd_pedido_parceiro = (df_pedidos\n",
    "                       .groupby(\"id_parceiro\")\n",
    "                       .agg(countDistinct(\"id_pedido\")\n",
    "                            .alias(\"pedidos_parceiro\"))\n",
    "                       .orderBy(\"pedidos_parceiro\", ascending=False))\n",
    "qtd_pedido_parceiro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.Quantidade de pedido por filial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|id_filial|pedidos_filial|\n",
      "+---------+--------------+\n",
      "|      231|        204263|\n",
      "|        3|        181948|\n",
      "|      257|         19380|\n",
      "|      883|         15514|\n",
      "|      228|         14119|\n",
      "|      366|         12327|\n",
      "|      494|         12214|\n",
      "|      117|         11741|\n",
      "|      547|          9831|\n",
      "|     1680|          7995|\n",
      "|      276|          7376|\n",
      "|     1692|          7083|\n",
      "|      439|          7056|\n",
      "|      229|          6937|\n",
      "|      435|          6728|\n",
      "|        4|          6270|\n",
      "|      141|          5961|\n",
      "|      224|          5471|\n",
      "|      884|          5136|\n",
      "|     1730|          3728|\n",
      "+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qtd_pedido_filial = (df_pedidos\n",
    "                     .groupby(\"id_filial\")\n",
    "                     .agg(countDistinct(\"id_pedido\")\n",
    "                          .alias(\"pedidos_filial\"))\n",
    "                     .orderBy(\"pedidos_filial\", ascending=False))\n",
    "qtd_pedido_filial.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.Juntar criar df_filial que deverá ser a junção das tabelas filial, cidade e estado;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filial.createOrReplaceTempView(\"filial\")\n",
    "df_cidade.createOrReplaceTempView(\"cidade\")\n",
    "df_estado.createOrReplaceTempView(\"estado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------------+-----------+---------+\n",
      "|id_filial|id_cidade|id_estado|      ds_filial|  ds_cidade|ds_estado|\n",
      "+---------+---------+---------+---------------+-----------+---------+\n",
      "|      805|      711|        7|Filial - 000805|   BARREIRA|       CE|\n",
      "|     1002|      711|        7|Filial - 001002|   BARREIRA|       CE|\n",
      "|     1233|      711|        7|Filial - 001233|   BARREIRA|       CE|\n",
      "|     1474|      986|        7|Filial - 001474|BREJO SANTO|       CE|\n",
      "|      325|      351|        7|Filial - 000325|    ARACATI|       CE|\n",
      "|      755|      351|        7|Filial - 000755|    ARACATI|       CE|\n",
      "|      800|      351|        7|Filial - 000800|    ARACATI|       CE|\n",
      "|      831|      351|        7|Filial - 000831|    ARACATI|       CE|\n",
      "|      863|      351|        7|Filial - 000863|    ARACATI|       CE|\n",
      "|      976|      351|        7|Filial - 000976|    ARACATI|       CE|\n",
      "|     1079|      351|        7|Filial - 001079|    ARACATI|       CE|\n",
      "|     1251|      351|        7|Filial - 001251|    ARACATI|       CE|\n",
      "|     1542|      351|        7|Filial - 001542|    ARACATI|       CE|\n",
      "|     1696|     1429|        7|Filial - 001696|   CARIDADE|       CE|\n",
      "|       92|     4216|        7|Filial - 000092|MORADA NOVA|       CE|\n",
      "|      203|       98|        7|Filial - 000203|     AIUABA|       CE|\n",
      "|      967|       98|        7|Filial - 000967|     AIUABA|       CE|\n",
      "|     1131|       98|        7|Filial - 001131|     AIUABA|       CE|\n",
      "|      154|       31|        7|Filial - 000154|   ACOPIARA|       CE|\n",
      "|      354|       31|        7|Filial - 000354|   ACOPIARA|       CE|\n",
      "+---------+---------+---------+---------------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT\n",
    "            F.id_filial,\n",
    "            C.id_cidade,\n",
    "            E.id_estado,\n",
    "            F.ds_filial,\n",
    "            C.ds_cidade,\n",
    "            E.ds_estado\n",
    "        FROM\n",
    "            filial AS F\n",
    "        JOIN\n",
    "            cidade AS C ON F.id_cidade = C.id_cidade\n",
    "        JOIN\n",
    "            estado AS E ON C.id_estado = E.id_estado\n",
    "        \"\"\"\n",
    "\n",
    "df_filiais = spark.sql(query)\n",
    "df_filiais.filter(df_filiais[\"id_filial\"] != \"id_filial\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.Juntar com o df_pedidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pedidos.createOrReplaceTempView(\"pedidos\")\n",
    "df_filiais.createOrReplaceTempView(\"filiais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+-----------+----------+---------+-------------+--------+----------+----------+-----------+---------+---------+----------------+---------+---------------+\n",
      "|id_pedido  |dt_pedido               |id_parceiro|id_cliente|id_filial|vr_total_pago|dt_foto |id_produto|quantidade|vr_unitario|id_cidade|id_estado|ds_cidade       |ds_estado|ds_filial      |\n",
      "+-----------+------------------------+-----------+----------+---------+-------------+--------+----------+----------+-----------+---------+---------+----------------+---------+---------------+\n",
      "|48443031713|2021-07-12T00:00:00.000Z|13         |136800692 |1090     |129.99       |20230603|4335308   |1         |129.99     |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48472409513|2021-07-12T00:00:00.000Z|13         |69267635  |1090     |259.99       |20230603|1375916   |1         |259.99     |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48454600516|2021-07-12T00:00:00.000Z|16         |48611362  |1090     |38.99        |20230603|1087204   |1         |38.99      |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48474015213|2021-07-12T00:00:00.000Z|13         |110762017 |1090     |181.99       |20230603|3007703   |1         |181.99     |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48433649513|2021-07-12T00:00:00.000Z|13         |114725005 |1090     |194.99       |20230603|2450167   |1         |194.99     |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48469345716|2021-07-12T00:00:00.000Z|16         |64085032  |1090     |106.59       |20230603|866295    |1         |106.59     |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48477626716|2021-07-13T00:00:00.000Z|16         |140176892 |1090     |131.23       |20230603|3429400   |1         |71.44      |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48477626716|2021-07-13T00:00:00.000Z|16         |140176892 |1090     |131.23       |20230603|4876008   |1         |59.79      |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48470836213|2021-07-12T00:00:00.000Z|13         |46721527  |1090     |389.98       |20230603|2120894   |1         |207.99     |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48470836213|2021-07-12T00:00:00.000Z|13         |46721527  |1090     |389.98       |20230603|1869081   |1         |181.99     |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48442153513|2021-07-12T00:00:00.000Z|13         |50397032  |1090     |45.49        |20230603|2780304   |1         |45.49      |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|48446389013|2021-07-12T00:00:00.000Z|13         |45217500  |1090     |103.99       |20230603|1160225   |1         |103.99     |737      |33       |BARUERI         |SP       |Filial - 001090|\n",
      "|49098483713|2021-07-25T00:00:00.000Z|13         |63531597  |1159     |71.49        |20230603|3190936   |1         |71.49      |234      |6        |AMELIA RODRIGUES|BA       |Filial - 001159|\n",
      "|49078822016|2021-07-25T00:00:00.000Z|16         |76511377  |1159     |90.99        |20230603|3134007   |1         |90.99      |234      |6        |AMELIA RODRIGUES|BA       |Filial - 001159|\n",
      "|49147908013|2021-07-26T00:00:00.000Z|13         |13997207  |1159     |259.99       |20230603|2380477   |1         |259.99     |234      |6        |AMELIA RODRIGUES|BA       |Filial - 001159|\n",
      "|4910176376 |2021-07-25T00:00:00.000Z|6          |54512730  |1159     |259.87       |20230603|287907    |1         |259.87     |234      |6        |AMELIA RODRIGUES|BA       |Filial - 001159|\n",
      "|49073085016|2021-07-25T00:00:00.000Z|16         |28884510  |1159     |131.27       |20230603|678641    |1         |44.19      |234      |6        |AMELIA RODRIGUES|BA       |Filial - 001159|\n",
      "|49073085016|2021-07-25T00:00:00.000Z|16         |28884510  |1159     |131.27       |20230603|1867788   |1         |44.19      |234      |6        |AMELIA RODRIGUES|BA       |Filial - 001159|\n",
      "|49073085016|2021-07-25T00:00:00.000Z|16         |28884510  |1159     |131.27       |20230603|2899106   |1         |42.89      |234      |6        |AMELIA RODRIGUES|BA       |Filial - 001159|\n",
      "|49074817016|2021-07-25T00:00:00.000Z|16         |4193435   |1159     |42.89        |20230603|2899106   |1         |42.89      |234      |6        |AMELIA RODRIGUES|BA       |Filial - 001159|\n",
      "+-----------+------------------------+-----------+----------+---------+-------------+--------+----------+----------+-----------+---------+---------+----------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT\n",
    "            P.*,\n",
    "            F.id_cidade,\n",
    "            F.id_estado,\n",
    "            F.ds_cidade,\n",
    "            F.ds_estado,\n",
    "            F.ds_filial\n",
    "        FROM\n",
    "            pedidos AS P\n",
    "        JOIN\n",
    "            filiais AS F ON P.id_filial = F.id_filial\n",
    "        \"\"\"\n",
    "\n",
    "df_pedidos_filiais = spark.sql(query)\n",
    "df_pedidos_filiais.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.Ver a quantidade de pedidos por estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+\n",
      "|id_estado|ds_estado|pedidos_por_estado|\n",
      "+---------+---------+------------------+\n",
      "|       14|       MG|            317571|\n",
      "|       18|       PA|            235021|\n",
      "|       11|       GO|             36273|\n",
      "|       20|       PE|             36204|\n",
      "|       31|       SC|             26252|\n",
      "|       33|       SP|             14683|\n",
      "|       13|       MA|             13833|\n",
      "|        6|       BA|             10541|\n",
      "|       28|       RS|              9024|\n",
      "|        7|       CE|              8042|\n",
      "|       22|       PR|              7315|\n",
      "|       25|       RN|              5419|\n",
      "|        3|       AL|              2822|\n",
      "|       10|       ES|              2308|\n",
      "|       32|       SE|              2202|\n",
      "|       19|       PB|              1992|\n",
      "|       21|       PI|              1213|\n",
      "|       24|       RJ|              1189|\n",
      "|       16|       MT|               675|\n",
      "|        8|       DF|               461|\n",
      "|       15|       MS|               270|\n",
      "|       35|       TO|               118|\n",
      "|       26|       RO|                39|\n",
      "|        4|       AM|                 3|\n",
      "|        2|       AC|                 1|\n",
      "+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pedidos_por_estado = (df_pedidos_filiais\n",
    "                      .groupBy(\"id_estado\", \"ds_estado\")\n",
    "                      .agg(countDistinct(\"id_pedido\").alias(\"pedidos_por_estado\"))\n",
    "                      .orderBy(\"pedidos_por_estado\", ascending=False))\n",
    "df_pedidos_por_estado.show(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.Top 10 filial que mais vendeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------+\n",
      "|id_filial|      ds_filial|volume_vendas|\n",
      "+---------+---------------+-------------+\n",
      "|      231|Filial - 000231|       240806|\n",
      "|        3|Filial - 000003|       220768|\n",
      "|      257|Filial - 000257|        23725|\n",
      "|      883|Filial - 000883|        17717|\n",
      "|      228|Filial - 000228|        15851|\n",
      "|      366|Filial - 000366|        13735|\n",
      "|      494|Filial - 000494|        13461|\n",
      "|      117|Filial - 000117|        13459|\n",
      "|      547|Filial - 000547|        11169|\n",
      "|     1680|Filial - 001680|         9531|\n",
      "+---------+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vendas_por_filial = (df_pedidos_filiais\n",
    "                     .groupBy(\"id_filial\", \"ds_filial\")\n",
    "                     .agg(round(sum(\"quantidade\"), 2).alias(\"volume_vendas\"))\n",
    "                     .orderBy(\"volume_vendas\", ascending=False))\n",
    "df_vendas_por_filial.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.Criar o dataframe df_stage juntando todas as bases do nosso modelo relacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pedidos com df_item_pedido\n",
    "df_stage = df_pedido.join(df_item_pedido, df_pedido.id_pedido == df_item_pedido.id_pedido, 'left')\n",
    "df_stage = df_stage.drop(df_item_pedido.id_pedido)\n",
    "\n",
    "#df_stage com df_parceiro\n",
    "df_stage = df_stage.join(df_parceiro, df_stage.id_parceiro == df_parceiro.id_parceiro, 'left')\n",
    "df_stage = df_stage.drop(df_parceiro.id_parceiro)\n",
    "\n",
    "#df_stage com df_cliente\n",
    "df_stage = df_stage.join(df_clientes, df_stage.id_cliente == df_clientes.id_cliente, 'left')\n",
    "df_stage = df_stage.drop(df_clientes.id_cliente)\n",
    "\n",
    "#df_stage com df_filial\n",
    "df_stage = df_stage.join(df_filial, df_stage.id_filial == df_filial.id_filial, 'left')\n",
    "df_stage = df_stage.drop(df_filial.id_filial)\n",
    "\n",
    "#df_stage com df_cidade\n",
    "df_stage = df_stage.join(df_cidade, df_stage.id_cidade == df_cidade.id_cidade, 'left')\n",
    "df_stage = df_stage.drop(df_cidade.id_cidade)\n",
    "\n",
    "#df_stage com df_estado\n",
    "df_stage = df_stage.join(df_estado, df_stage.id_estado == df_estado.id_estado, 'left')\n",
    "df_stage = df_stage.drop(df_estado.id_estado)\n",
    "\n",
    "# df_stage com df_produtos\n",
    "#df_stage = df_stage.join(df_produtos, df_stage.ds_produto == df_produtos.ds_produto, 'left')\n",
    "#df_stage = df_stage.drop(df_produtos.ds_produto)\n",
    "\n",
    "# df_stage com df_subcategoria\n",
    "#df_stage = df_stage.join(df_subcategoria, df_stage.id_subcategoria == df_subcategoria.id_subcategoria, 'left')\n",
    "#df_stage = df_stage.drop(df_subcategoria.id_subcategoria)\n",
    "\n",
    "# df_stage com df_categorias\n",
    "#df_stage = df_stage.join(df_categorias, df_stage.id_categoria == df_categorias.id_categoria, 'left')\n",
    "#df_stage = df_stage.drop(df_categorias.id_categoria)\n",
    "\n",
    "#show\n",
    "#df_stage.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalhando com calendario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Com o dataframe df_stage criado, criar as colunas do calendário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o740.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 112 (showString at <unknown>:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Stream is corrupted \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:470) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:794) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:755) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:917) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:953) \tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) \tat org.apache.spark.scheduler.Task.run(Task.scala:121) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Stream is corrupted \tat net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:202) \tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) \tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:170) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:361) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.util.Utils$.copyStream(Utils.scala:369) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:462) \t... 31 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor134.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ecbe4e065172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m            )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_stage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o740.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 112 (showString at <unknown>:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Stream is corrupted \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:470) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage16.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:794) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:755) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:917) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:953) \tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) \tat org.apache.spark.scheduler.Task.run(Task.scala:121) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Stream is corrupted \tat net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:202) \tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) \tat net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:170) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:361) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348) \tat org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.util.Utils$.copyStream(Utils.scala:369) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:462) \t... 31 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor134.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df_stage = (df_stage\n",
    "            .withColumn('Ano', year(df_stage.dt_pedido))\n",
    "            .withColumn('Mês', month(df_stage.dt_pedido))\n",
    "            .withColumn('Dia', dayofmonth(df_stage.dt_pedido))\n",
    "            .withColumn('Trimestre', quarter(df_stage.dt_pedido))\n",
    "           )\n",
    "\n",
    "df_stage.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---+---+---------+\n",
      "|           dt_pedido| Ano|Mês|Dia|Trimestre|\n",
      "+--------------------+----+---+---+---------+\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "|2021-06-02T00:00:...|2021|  6|  2|        2|\n",
      "+--------------------+----+---+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#apenas para mostrar apenas o que foi pedido na atividade\n",
    "df_stage.select('dt_pedido', 'Ano', 'Mês', 'Dia', 'Trimestre').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
