{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação das tabelas dimensionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/3.1.1/api/python/reference/index.html\n",
    "from pyspark.sql import SparkSession, dataframe\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as f\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dataframes diretamente do Hive\n",
    "\n",
    "# Configuração da base que será utilizada\n",
    "DB_NAME = \"prd_magalu_db\"\n",
    "FS_GOLD_DIR = \"/input/curso_minsait/magalu_project/gold\"\n",
    "HDFS_BASE_DIR = \"datalake/magalu_project\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_categoria e dim_produto\n",
    "df_categoria = spark.sql(f\"select * from {DB_NAME}.categoria\")\n",
    "df_produto = spark.sql(f\"select * from {DB_NAME}.produto\")\n",
    "df_subcategoria = spark.sql(f\"select * from {DB_NAME}.subcategoria\")\n",
    "\n",
    "# dim_filial e dim_localidade\n",
    "df_cidade = spark.sql(f\"select * from {DB_NAME}.cidade\")\n",
    "df_cliente = spark.sql(f\"select * from {DB_NAME}.cliente\")\n",
    "df_estado = spark.sql(f\"select * from {DB_NAME}.estado\")\n",
    "df_filial = spark.sql(f\"select * from {DB_NAME}.filial\")\n",
    "\n",
    "# fato_pedido e dim_calendario\n",
    "df_pedido = spark.sql(f\"select * from {DB_NAME}.pedido\")\n",
    "df_item_pedido = spark.sql(f\"select * from {DB_NAME}.item_pedido\")\n",
    "\n",
    "# dim_parceiro\n",
    "df_parceiro = spark.sql(f\"select * from {DB_NAME}.parceiro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espaço para tratar e juntar os campos e a criação do modelo dimensional\n",
    "\n",
    "df_pedido.createOrReplaceTempView('pedido')\n",
    "df_item_pedido.createOrReplaceTempView('item_pedido')\n",
    "\n",
    "df_categoria.createOrReplaceTempView('categoria')\n",
    "df_produto.createOrReplaceTempView('produto')\n",
    "df_subcategoria.createOrReplaceTempView('subcategoria')\n",
    "\n",
    "df_cidade.createOrReplaceTempView('cidade')\n",
    "df_cliente.createOrReplaceTempView('cliente')\n",
    "df_estado.createOrReplaceTempView('estado')\n",
    "df_filial.createOrReplaceTempView('filial')\n",
    "\n",
    "df_parceiro.createOrReplaceTempView('parceiro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    select \n",
    "        p.id_pedido,\n",
    "        p.dt_pedido,\n",
    "        p.id_parceiro,\n",
    "        p.id_cliente,\n",
    "        p.id_filial,\n",
    "        p.vr_total_pago,\n",
    "        ip.id_produto,\n",
    "        ip.quantidade,\n",
    "        ip.vr_unitario,\n",
    "        prd.ds_produto,\n",
    "        sc.id_subcategoria,\n",
    "        sc.ds_subcategoria,\n",
    "        c.id_categoria,\n",
    "        c.ds_categoria,\n",
    "        c.perc_parceiro,\n",
    "        cl.nm_cliente,\n",
    "        cl.flag_ouro,\n",
    "        parc.nm_parceiro,\n",
    "        f.ds_filial,\n",
    "        f.id_cidade,\n",
    "        cid.ds_cidade,\n",
    "        cid.id_estado,\n",
    "        est.ds_estado        \n",
    "    from pedido as p \n",
    "    inner join item_pedido as ip\n",
    "    on p.id_pedido == ip.id_pedido\n",
    "    left join produto as prd on ip.id_produto == prd.id_produto\n",
    "    left join subcategoria  sc on sc.id_subcategoria == prd.id_subcategoria\n",
    "    left join categoria  c on c.id_categoria == sc.id_categoria\n",
    "    left join cliente  cl on cl.id_cliente == p.id_cliente\n",
    "    left join filial  f on p.id_filial == f.id_filial\n",
    "    left join cidade  cid on cid.id_cidade == f.id_cidade\n",
    "    left join estado  est on est.id_estado == cid.id_estado\n",
    "    left join parceiro  parc on parc.id_parceiro == p.id_parceiro\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da STAGE\n",
    "df_stage = spark.sql(sql)\n",
    "\n",
    "# Criação dos Campos Calendario\n",
    "df_stage = (df_stage\n",
    "            .withColumn('Ano', year(df_stage.dt_pedido))\n",
    "            .withColumn('Mes', month(df_stage.dt_pedido))\n",
    "            .withColumn('Dia', dayofmonth(df_stage.dt_pedido))\n",
    "            .withColumn('Trimestre', quarter(df_stage.dt_pedido))\n",
    "           )\n",
    "\n",
    "# Criação das Chaves do Modelo\n",
    "\n",
    "df_stage = df_stage.withColumn(\"DW_PRODUTO\", sha2(concat_ws(\"\", df_stage.ds_produto, df_stage.id_produto), 256))\n",
    "df_stage = df_stage.withColumn(\"DW_CLIENTE\", sha2(concat_ws(\"\", df_stage.nm_cliente, df_stage.flag_ouro, df_stage.id_cliente), 256))\n",
    "df_stage = df_stage.withColumn(\"DW_PARCEIRO\", sha2(concat_ws(\"\", df_stage.id_parceiro, df_stage.nm_parceiro), 256))\n",
    "df_stage = df_stage.withColumn(\"DW_FILIAL\", sha2(concat_ws(\"\", df_stage.id_filial, df_stage.ds_filial), 256))\n",
    "df_stage = df_stage.withColumn(\"DW_TEMPO\", sha2(concat_ws(\"\", df_stage.dt_pedido, df_stage.Ano, df_stage.Mes, df_stage.Dia), 256))\n",
    "df_stage = df_stage.withColumn(\"DW_LOCALIDADE\", sha2(concat_ws(\"\", df_stage.id_estado, df_stage.id_cidade, df_stage.ds_estado, df_stage.ds_cidade), 256))\n",
    "df_stage = df_stage.withColumn(\"DW_CATEGORIA\", sha2(concat_ws(\"\", df_stage.id_categoria, df_stage.id_subcategoria, df_stage.ds_categoria, df_stage.ds_subcategoria), 256))\n",
    "\n",
    "df_stage.createOrReplaceTempView('stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a dimensão Cliente\n",
    "dim_cliente = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_CLIENTE,\n",
    "        id_cliente,\n",
    "        nm_cliente,\n",
    "        flag_ouro\n",
    "    FROM stage    \n",
    "''')\n",
    "\n",
    "#Criando a dimensão Produto\n",
    "dim_produto = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_PRODUTO,\n",
    "        id_produto,\n",
    "        ds_produto\n",
    "    FROM stage    \n",
    "''')\n",
    "\n",
    "#Criando a dimensão Tempo\n",
    "dim_tempo = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_TEMPO,\n",
    "        dt_pedido,\n",
    "        Ano,\n",
    "        Mes,\n",
    "        Dia\n",
    "    FROM stage    \n",
    "''')\n",
    "\n",
    "#Criando a dimensão Localidade\n",
    "dim_localidade = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_LOCALIDADE,\n",
    "        id_estado,\n",
    "        id_cidade,\n",
    "        ds_estado,\n",
    "        ds_cidade\n",
    "    FROM stage    \n",
    "''')\n",
    "\n",
    "#Criando a dimensão filial\n",
    "dim_filial = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_FILIAL,\n",
    "        id_filial,\n",
    "        ds_filial\n",
    "    FROM stage    \n",
    "''')\n",
    "\n",
    "#Criando a dimensão Parceiro\n",
    "dim_parceiro = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_PARCEIRO,\n",
    "        id_parceiro,\n",
    "        nm_parceiro\n",
    "    FROM stage    \n",
    "''')\n",
    "\n",
    "#Criando a dimensão Categoria\n",
    "dim_categoria = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        DW_CATEGORIA,\n",
    "        id_categoria,\n",
    "        id_subcategoria,\n",
    "        ds_categoria,\n",
    "        ds_subcategoria\n",
    "    FROM stage    \n",
    "''')\n",
    "\n",
    "#Criando a Fato Pedidios\n",
    "ft_pedidos = spark.sql('''\n",
    "    SELECT \n",
    "        DW_PARCEIRO,\n",
    "        DW_CLIENTE,\n",
    "        DW_FILIAL,\n",
    "        DW_LOCALIDADE,\n",
    "        DW_TEMPO,\n",
    "        sum(vr_total_pago) as vl_total\n",
    "    FROM stage\n",
    "    group by \n",
    "        DW_PARCEIRO,\n",
    "        DW_CLIENTE,\n",
    "        DW_FILIAL,\n",
    "        DW_LOCALIDADE,\n",
    "        DW_TEMPO\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferindo dados: /input/curso_minsait/magalu_project/gold/ft_pedidos\n",
      "root\n",
      " |-- DW_PARCEIRO: string (nullable = true)\n",
      " |-- DW_CLIENTE: string (nullable = true)\n",
      " |-- DW_FILIAL: string (nullable = true)\n",
      " |-- DW_LOCALIDADE: string (nullable = true)\n",
      " |-- DW_TEMPO: string (nullable = true)\n",
      " |-- vl_total: double (nullable = true)\n",
      "\n",
      "hdfs dfs -rm /input/curso_minsait/magalu_project/gold/ft_pedidos/*\n",
      "hdfs dfs -get datalake/magalu_project/gold/ft_pedidos/part-* /input/curso_minsait/magalu_project/gold/ft_pedidos.csv\n",
      "Sucesso para ft_pedidos\n",
      "Transferindo dados: /input/curso_minsait/magalu_project/gold/dim_cliente\n",
      "root\n",
      " |-- DW_CLIENTE: string (nullable = true)\n",
      " |-- id_cliente: string (nullable = true)\n",
      " |-- nm_cliente: string (nullable = true)\n",
      " |-- flag_ouro: string (nullable = true)\n",
      "\n",
      "hdfs dfs -rm /input/curso_minsait/magalu_project/gold/dim_cliente/*\n",
      "hdfs dfs -get datalake/magalu_project/gold/dim_cliente/part-* /input/curso_minsait/magalu_project/gold/dim_cliente.csv\n",
      "Sucesso para dim_cliente\n",
      "Transferindo dados: /input/curso_minsait/magalu_project/gold/dim_tempo\n",
      "root\n",
      " |-- DW_TEMPO: string (nullable = true)\n",
      " |-- dt_pedido: string (nullable = true)\n",
      " |-- Ano: integer (nullable = true)\n",
      " |-- Mes: integer (nullable = true)\n",
      " |-- Dia: integer (nullable = true)\n",
      "\n",
      "hdfs dfs -rm /input/curso_minsait/magalu_project/gold/dim_tempo/*\n",
      "hdfs dfs -get datalake/magalu_project/gold/dim_tempo/part-* /input/curso_minsait/magalu_project/gold/dim_tempo.csv\n",
      "Sucesso para dim_tempo\n",
      "Transferindo dados: /input/curso_minsait/magalu_project/gold/dim_localidade\n",
      "root\n",
      " |-- DW_LOCALIDADE: string (nullable = true)\n",
      " |-- id_estado: string (nullable = true)\n",
      " |-- id_cidade: string (nullable = true)\n",
      " |-- ds_estado: string (nullable = true)\n",
      " |-- ds_cidade: string (nullable = true)\n",
      "\n",
      "hdfs dfs -rm /input/curso_minsait/magalu_project/gold/dim_localidade/*\n",
      "hdfs dfs -get datalake/magalu_project/gold/dim_localidade/part-* /input/curso_minsait/magalu_project/gold/dim_localidade.csv\n",
      "Sucesso para dim_localidade\n",
      "Transferindo dados: /input/curso_minsait/magalu_project/gold/dim_produto\n",
      "root\n",
      " |-- DW_PRODUTO: string (nullable = true)\n",
      " |-- id_produto: string (nullable = true)\n",
      " |-- ds_produto: string (nullable = true)\n",
      "\n",
      "hdfs dfs -rm /input/curso_minsait/magalu_project/gold/dim_produto/*\n",
      "hdfs dfs -get datalake/magalu_project/gold/dim_produto/part-* /input/curso_minsait/magalu_project/gold/dim_produto.csv\n",
      "Sucesso para dim_produto\n",
      "Transferindo dados: /input/curso_minsait/magalu_project/gold/dim_parceiro\n",
      "root\n",
      " |-- DW_PARCEIRO: string (nullable = true)\n",
      " |-- id_parceiro: string (nullable = true)\n",
      " |-- nm_parceiro: string (nullable = true)\n",
      "\n",
      "hdfs dfs -rm /input/curso_minsait/magalu_project/gold/dim_parceiro/*\n",
      "hdfs dfs -get datalake/magalu_project/gold/dim_parceiro/part-* /input/curso_minsait/magalu_project/gold/dim_parceiro.csv\n",
      "Sucesso para dim_parceiro\n",
      "Transferindo dados: /input/curso_minsait/magalu_project/gold/dim_categoria\n",
      "root\n",
      " |-- DW_CATEGORIA: string (nullable = true)\n",
      " |-- id_categoria: string (nullable = true)\n",
      " |-- id_subcategoria: string (nullable = true)\n",
      " |-- ds_categoria: string (nullable = true)\n",
      " |-- ds_subcategoria: string (nullable = true)\n",
      "\n",
      "hdfs dfs -rm /input/curso_minsait/magalu_project/gold/dim_categoria/*\n",
      "hdfs dfs -get datalake/magalu_project/gold/dim_categoria/part-* /input/curso_minsait/magalu_project/gold/dim_categoria.csv\n",
      "Sucesso para dim_categoria\n"
     ]
    }
   ],
   "source": [
    "# função para salvar os dados\n",
    "def salvar_df(_df, _file):\n",
    "    \n",
    "#     DB_NAME = \"prd_magalu_db\"\n",
    "#     FS_GOLD_DIR = \"/input/curso_minsait/magalu_project/gold\"\n",
    "#     HDFS_BASE_DIR = \"/datalake/magalu_project\"\n",
    "\n",
    "    try:\n",
    "        output = f\"{FS_GOLD_DIR}/{_file}\"\n",
    "        erase = f\"hdfs dfs -rm {output}/*\"\n",
    "        rename = f\"hdfs dfs -get {HDFS_BASE_DIR}/gold/{_file}/part-* {FS_GOLD_DIR}/{_file}.csv\"\n",
    "    \n",
    "        print(f\"Transferindo dados: {output}\")\n",
    "        _df.printSchema()\n",
    "        \n",
    "        _df.coalesce(1).write\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"header\", True)\\\n",
    "            .option(\"delimiter\", \";\")\\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .save(f\"{HDFS_BASE_DIR}/gold/{_file}/\")\n",
    "\n",
    "        print(erase)\n",
    "        os.system(erase)\n",
    "\n",
    "        print(rename)\n",
    "        os.system(rename)\n",
    "        \n",
    "        print(f\"Sucesso para {_file}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "salvar_df(ft_pedidos, 'ft_pedidos')\n",
    "salvar_df(dim_cliente, 'dim_cliente')\n",
    "salvar_df(dim_tempo, 'dim_tempo')\n",
    "salvar_df(dim_localidade, 'dim_localidade')\n",
    "salvar_df(dim_produto, 'dim_produto')\n",
    "salvar_df(dim_parceiro, 'dim_parceiro')\n",
    "salvar_df(dim_categoria, 'dim_categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_parquet(\"/datalake/magalu_project/gold/parquet/dim_parceiro.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
